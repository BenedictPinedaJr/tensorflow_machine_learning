{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to neural network classification with TensorFlow\n",
    "\n",
    "Learning how to write neural networks for classification problems\n",
    "\n",
    "Classification is identifying something as one thing or another.\n",
    "\n",
    "Some types of classification problems:\n",
    "* Binary Classificatoin\n",
    "* Multiclass Classification\n",
    "* Multilabel Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data to view and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Make 1000 samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples,\n",
    "                    noise=0.03,\n",
    "                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out features\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the labels\n",
    "y[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "circles = pl.DataFrame({\"X0\":X[:, 0], \"X1\": X[:, 1], \"label\": y})\n",
    "circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with a plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes of features and lables\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first sample of features and labels\n",
    "X[0], y[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in modelling\n",
    "\n",
    "1. Create or import a model\n",
    "2. Compile the model\n",
    "3. Fit the model\n",
    "4. Evaluate the model\n",
    "5. Tweak the model\n",
    "6. Evaluate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model using the Sequential API\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_1.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "model_1.fit(X, y, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve model by training longer\n",
    "model_1.fit(X, y, epochs=50, verbose=0)\n",
    "model_1.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra layer to new model\n",
    "\n",
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model using the Sequential API\n",
    "model_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1),\n",
    "    tf.keras.layers.Dense(1), \n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.SGD(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "model_2.fit(X, y, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the model\n",
    "model_2.evaluate(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving model\n",
    "\n",
    "1. Create a model - add more layers, increase number of hidden units in a layer, change or add activation layer\n",
    "2. Compiling a model - Choose a different optimization function such as Adam instead of SGD\n",
    "3. Fitting a model - fit model with more epochs (training for longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model using the Sequential API (with 3 layers)\n",
    "model_3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100), # Add 100 dense neurons\n",
    "    tf.keras.layers.Dense(10), # Add another layer with 10 neurons\n",
    "    tf.keras.layers.Dense(1) \n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "model_3.fit(X, y, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the model\n",
    "model_3.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.predict(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the model's predicitons. Create a function `plot_decision_boundary`, this function will:\n",
    "\n",
    "* Take in a trained model, features (X) and labels (y)\n",
    "* Create a meshgrid of the different X values\n",
    "* Make predictions across the meshgrid\n",
    "* Plot the prediction as well as a line between zones (where each unique class falls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"\n",
    "    Plots the decision boundary created by a model predicting on X.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the axis boundaries of the plot and create a meshgrid\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Create X value (make predictions on these)\n",
    "    x_in = np.c_[xx.ravel(), yy.ravel()] # Stack 2D arrays together\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(x_in)\n",
    "\n",
    "    # Check for multiclass\n",
    "    if len(y_pred[0]) > 1:\n",
    "        print(\"doing multiclass classification\")\n",
    "        # Reshape predictions for plotting\n",
    "        y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n",
    "    else: \n",
    "        print(\"doing binary classification\")\n",
    "        y_pred = np.round(y_pred).reshape(xx.shape)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu) \n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "plot_decision_boundary(model=model_3, X=X, y=y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model_4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\") \n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "history = model_4.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the decision boundary for latest model\n",
    "plot_decision_boundary(model_4, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build neural network with a non-linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model using a non-linear activation\n",
    "model_5 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, activation=\"relu\") \n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_5.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "history = model_5.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another model with more layers\n",
    "\n",
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model \n",
    "model_6 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"), \n",
    "    tf.keras.layers.Dense(1), \n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_6.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "history = model_6.fit(X, y, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary\n",
    "plot_decision_boundary(model_6, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model \n",
    "model_7 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"), \n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"), \n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_7.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "history = model_7.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate model\n",
    "model_7.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(model_7, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of **linear (straight lines) and non-linear (non-straight lines) functions** is one of the key fundamentals of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy tensor \n",
    "A = tf.cast(tf.range(-10, 10), tf.float32)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize toy tensor\n",
    "plt.plot(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate sigmoid(x) = 1 / (1 + exp(-x))\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + tf.exp(-x))\n",
    "\n",
    "# Use sigmoid function\n",
    "sigmoid(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tensor transformed by sigmoid\n",
    "plt.plot(sigmoid(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate relu function\n",
    "def relu(x):\n",
    "    return tf.maximum(0, x)\n",
    "\n",
    "# Pass toy tensor to relu function\n",
    "relu(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tensor transformed by ReLU\n",
    "plt.plot(relu(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.activations.linear(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tf.keras.activations.linear(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating and improving classification model\n",
    "\n",
    "Create a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many samples\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model that fits on the training data and evaluated using the testing data\n",
    "\n",
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model (same as model_7)\n",
    "model_8 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# 2. Compile the model (increase learning rate)\n",
    "model_8.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model (lower number of epochs)\n",
    "history = model_8.fit(X_train, y_train, epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the model (must be using test data)\n",
    "model_8.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model=model_8, X=X_train, y=y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model=model_8, X=X_test, y=y_test)\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss (or training) curvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert history object to a dataframe\n",
    "pl.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "plt.plot(pl.DataFrame(history.history))\n",
    "plt.legend([\"accuracy\",\"loss\"])\n",
    "plt.title(\"Model 8 loss curves\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many problems, the loss function going down means the model is improving (the predictions it's making are getting close to the ground truth labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best learning rate\n",
    "\n",
    "To find the ideal learning rate (the learning rate where the loss decreatese the most during training) use the following steps:\n",
    "\n",
    "* A learning rate **callback** - extra piece of functionality that can be added while training model\n",
    "* Another model \n",
    "* Modified loss curves plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model_9 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_9.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=\"Adam\",\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Create a learning rate callback\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))\n",
    "\n",
    "# 4. Fit the model (pass lr_scheduler)\n",
    "history_9 = model_9.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs=100, \n",
    "                      callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkout history\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(pl.DataFrame(history_9.history))\n",
    "plt.legend([\"loss\", \"accuracy\", \"lr\"])\n",
    "plt.xlabel(\"epochs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkout the learning rate versus the loss\n",
    "lrs = 1e-4 * (10 ** (tf.range(100)/200))\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.semilogx(lrs, history_9.history[\"loss\"])\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning rate vs. Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of other typical learning rates values:\n",
    "10**0, 10**-1, 10**-2, 10**-3, 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using a higher ideal learing rate with the same model\n",
    "\n",
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model_10 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_10.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.02),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model (pass lr_scheduler)\n",
    "history_10 = model_10.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate model 10\n",
    "model_10.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model 8 \n",
    "model_8.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundaries for the training and test sets\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_10, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_10, X_test, y_test)\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More classification evaluation methods\n",
    "\n",
    "* Accuracy\n",
    "* Precision \n",
    "* Recall\n",
    "* F1-score \n",
    "* Confusion Matrix\n",
    "* Classification Report (Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model accuracy\n",
    "loss, accuracy = model_10.evaluate(X_test, y_test)\n",
    "print(f\"Model loss: {loss}\")\n",
    "print(f\"Model accuracy: {(accuracy*100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Make prediction (Convert prediction probabilites to binary predictions)\n",
    "y_preds = tf.round(model_10.predict(X_test))\n",
    "\n",
    "# Create and plot confusion matrix\n",
    "cm = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_preds))\n",
    "cm.plot(cmap=\"Blues\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a larger example (Multiclass classification)\n",
    "\n",
    "When there are more than two classes of an option, it's known as **multi-class classification**\n",
    "\n",
    "* This means having 3 different classes is a multiclass-classifcation.\n",
    "* Also means that having 100 different classes is also a mutliclass=classification.\n",
    "\n",
    "To practice multiclass classification, build a neural network to classify images of different items of clothin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# The data has already been splitted into training and test sets\n",
    "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first training example\n",
    "print(f\"Training sample:\\n{train_data[0]}\\n\")\n",
    "print(f\"Training label:\\n{train_labels[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of a single example\n",
    "train_data[0].shape, train_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_data[8]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out samples label\n",
    "train_labels[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small list to index training labels to become human readable\n",
    "class_names = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample image and label\n",
    "chosen_index = 20\n",
    "plt.imshow(train_data[chosen_index], cmap=plt.cm.binary)\n",
    "plt.title(class_names[train_labels[chosen_index]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multiple random images of fashion MNISt\n",
    "import random\n",
    "plt.figure(figsize=(7, 7))\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(2, 2, i+1)\n",
    "    rand_index = random.choice(range(len(train_data)))\n",
    "    plt.imshow(train_data[rand_index], cmap=plt.cm.binary)\n",
    "    plt.title(class_names[train_labels[rand_index]])\n",
    "    plt.axis(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a multiclass classification model\n",
    "\n",
    "For the multiclass classification model, a similar architecture of binary classification can be followed but a few things need to be tweaked:\n",
    "\n",
    "* Input shape = 28 x 28 (the shape of one image)\n",
    "* Output shape = 10 (one per class of clothing)\n",
    "* Loss function = tf.keras.losses.CategoricalCrossentropy()\n",
    "  * If labels are one-hot encoded use CategoricalCrossEntropy() but if labels are in integer form use SparseCategoricalEntropy()\n",
    "* Output layer activation = Softmax (not Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model_11 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # data needs to be flattend to a vector\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_11.compile(loss=\"categorical_crossentropy\",\n",
    "                 optimizer=tf.keras.optimizers.Adam(),\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "non_norm_history = model_11.fit(train_data,\n",
    "                                tf.one_hot(train_labels, depth=10),\n",
    "                                epochs=10,\n",
    "                                validation_data=(test_data, tf.one_hot(test_labels, depth=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model summary\n",
    "model_11.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the min and max values of the training dataf\n",
    "train_data.min(), train_data.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks prefer data to be scaled (or normalized), this means that they prefer to have the numbers in the tensors to be between 0 & 1 to find patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data between 0 and 1 by dividing by the maximum\n",
    "train_data_norm = train_data / 255.0\n",
    "test_data_norm = test_data / 255.0\n",
    "\n",
    "# Check the min and max values of the scaled training data\n",
    "train_data_norm.min(), train_data_norm.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model_12 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_12.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=tf.keras.optimizers.Adam(),\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "norm_history = model_12.fit(train_data_norm,\n",
    "                                train_labels,\n",
    "                                epochs=10,\n",
    "                                validation_data=(test_data_norm, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "# Plot non-normalized data loss curves\n",
    "plt.plot(pl.DataFrame(non_norm_history.history))\n",
    "plt.legend(pl.DataFrame(non_norm_history.history).columns)\n",
    "plt.title(\"Non-normalized data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized data loss curves\n",
    "plt.plot(pl.DataFrame(norm_history.history))\n",
    "plt.legend(pl.DataFrame(norm_history.history).columns)\n",
    "plt.title(\"Normalized data\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same model with even *slightly* different data can produce *dramatically* different results. So when comparing models, it's important to make sure they are being compared on the same criteria (e.g. same architecture but different data or same data but different architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the ideal learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model_13 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_13.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=tf.keras.optimizers.Adam(),\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "# Create the learning rate callback\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))\n",
    "\n",
    "# 3. Fit the model\n",
    "find_lr_history = model_13.fit(train_data_norm,\n",
    "                                train_labels,\n",
    "                                epochs=40,\n",
    "                                validation_data=(test_data_norm, test_labels),\n",
    "                                callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning rate decay curve\n",
    "lrs = 1e-3 * (10**(tf.range(40)/20))\n",
    "plt.semilogx(lrs, find_lr_history.history[\"loss\"])\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Finding the ideal learning rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit model\n",
    "\n",
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model_14 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_14.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "history_14 = model_14.fit(train_data_norm,\n",
    "                                train_labels,\n",
    "                                epochs=20,\n",
    "                                validation_data=(test_data_norm, test_labels))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating multiclass classification model\n",
    "\n",
    "* Evaluate its performance using other classification metrics\n",
    "* Assess some of its predictions (through visualizations)\n",
    "* Improve its results (by trainin it for longer or changing the architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Make prediction (Convert prediction probabilites to interger predictions)\n",
    "predictions = model_14.predict(test_data_norm).argmax(axis=1)\n",
    "\n",
    "# Create and plot confusion matrix\n",
    "cm = ConfusionMatrixDisplay.from_predictions(test_labels, predictions, \n",
    "                            display_labels=class_names,\n",
    "                            cmap=\"Blues\")\n",
    "cm.figure_.set_figwidth(10)\n",
    "cm.figure_.set_figheight(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function for:\n",
    "* Plotting a random image\n",
    "* Making a prediction on said image\n",
    "* Label the plot with the truth label and the predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def plot_random_image(model, images, true_labels, classes):\n",
    "    \"\"\"\n",
    "    Picks a random image, plots it and labels it with a prediciton and truth label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up random integer\n",
    "    i = random.randint(0, len(images))\n",
    "\n",
    "    # Create predictions and targets\n",
    "    target_image = images[i]\n",
    "    pred_probs = model.predict(target_image.reshape(1, 28, 28))\n",
    "    pred_label = classes[pred_probs.argmax()]\n",
    "    true_label = classes[true_labels[i]]\n",
    "\n",
    "    # Plot the image\n",
    "    plt.imshow(target_image, cmap=plt.cm.binary)\n",
    "\n",
    "    # Change the color of the titles depending if the prediction is right or wrong\n",
    "    if pred_label == true_label:\n",
    "        color = \"green\"\n",
    "    else:\n",
    "        color = \"red\"\n",
    "\n",
    "    # Add xlabel information (prediction/true label)\n",
    "    plt.xlabel(\"Pred: {} {:2.0f}% True: {}\".format(pred_label, \n",
    "                                                     100*tf.reduce_max(pred_probs),\n",
    "                                                     true_label),\n",
    "                                                     color=color) # set the color to green or red depending on prediction        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out a random image as well as its prediction\n",
    "plot_random_image(model_14, \n",
    "                  images=test_data_norm, \n",
    "                  true_labels=test_labels, \n",
    "                  classes=class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What patterns is the model learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the layers of the most recent model\n",
    "model_14.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a particular layer\n",
    "model_14.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the patterns of a layer in the network\n",
    "weights, biases = model_14.layers[1].get_weights()\n",
    "\n",
    "# Shapes\n",
    "weights, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_14.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases, biases.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every neuron has a bias vector. Each of these is paired with a weights matrix.\n",
    "\n",
    "The bias vector get initialized as zeroes.\n",
    "\n",
    "The bias vector dictates how much the patterns within the corresponding weights matrix should influence the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out another way of viewing deep learning models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# See the inputs and outputs of each layer\n",
    "plot_model(model_14, show_shapes=True, to_file=\"models/model_14_layers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
